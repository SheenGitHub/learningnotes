# 主题模型
***
## Unigram Model
这个模型假设所有文档都只有一个主题，生成文档时，都以一定的概率从这个唯一的“主题——词”分布中抽取

> N代表词的个数，M代表文档数目，w代表文档中的词，所以，我们生成一篇文档的概率就是：
![](http://ww4.sinaimg.cn/mw690/48ceb85dgw1fb552447sbj208c02ra9z.jpg)
黑体的w代表文档的词向量

***

## Mixture of Unigram

跟unigram相比，mixture of unigram增加了一层主题层，也就是说，我们在“生成文档”时，有了更多的主题选择，选择了主题之后，再从对应的“主题－词”分布中生成文档中所有的词，虽然更合理了一些，但是从一个主题中生成一篇文档实在是有些牵强

> z代表我们所选择的主题，现在生成一篇文档的概率就变成了下面的样子：
![](http://ww4.sinaimg.cn/mw690/48ceb85dgw1fb5576qcuuj208h02aglm.jpg)

***

## plSA(Probabilistic LatentSemantic Analysis)
从一个主题中生成整篇文档的做法实在是太牵强了，所以，Hoffman在1999年的时候提出了一个从多个主题中生成文档中的词的模型，就是pLSA

Hoffman认为，一篇文档可以由多个主题混合而成，而每个主题都是词汇上的概率分布，而文章中的每一个词都是由一个固定的主题生成

> 假设我现在要开始着手写一篇关于“自然语言处理”的文章
那我现在就有40%的概率去谈论语言学，30%的概率去谈论概率统计，20%的概率去谈论计算机，还有10%的可能去谈论其他的主题，主题层OK了，还得考虑词

```
说到语言学，我们容易想到的词包括：语法，句子，乔姆斯基，句法分析，主语  
谈论概率统计，我们容易想到一些词：概率，模型，均值，方差，证明，独立，马尔可夫链  
谈论计算机，我们容易想到的词是：内存，硬盘，变成，二进制，对象，算法，复杂度 
```

> 对于文档中的每个词而言，首先以一定的概率选择主题，再从主题－词分布中生成我们需要的词，现在，生成整篇文档的概率就变成了如下：
> ![](http://ww1.sinaimg.cn/large/48ceb85dgw1fb55m9kpw1j207g01njrd.jpg)
dm代表语料库中的第m篇文档，K代表主题的个数，p(wi|z)代表给定主题z，生成词wi的概率；p(z|dm)代表给定文档dm，我们选择主题z的概率

***
## LDA

LDA的作者认为，“文档－主题”分布与“主题－词”分布本来就是一个随机变量，随机变量就应该要有先验分布，考虑到“主题－词”和“文档－主题”分布都是多项分布，所以作者选择了与多项分布互为共轭分布的狄利克雷分布作为先验分布

> alpha和beta是狄利克雷先验分布的参数，假设现在我们准备生成文档d, 具体的物理过程就变成了这样：

> 1.选择满足狄利克雷先验分布的“文档－主题”分布theta， theta实际上就是一个|D|*|V|的矩阵，矩阵的元素对应着每个主题在对应文档下出现的概率；然后抽取我们需要的主题

> 2.选择满足狄利克雷先验分布的“主题－词”分布phi，对应的phi是一个|V|*|W|的矩阵；根据1中抽取的主题，选择我们将要生成词的多项分布，然后再以一定的概率生成词
> ![](http://ww3.sinaimg.cn/mw690/48ceb85dgw1fb5638jkgyj20ad01v3ym.jpg)

>p(w|theta,beta)表示的是在给定主题theta和超参数（狄利克雷分布的参数）beta之后，生成w的概率，这里的thete应该要同刚才写到的“文档－主题”分布theta区别开，这里的theta可以理解为矩阵的一行，p(z|theta)就代表给定文档－主题分布之后，选择主题z的概率，p(w|z,beta)表示的是给定主题z和超参数beta之后，生成词w的概率